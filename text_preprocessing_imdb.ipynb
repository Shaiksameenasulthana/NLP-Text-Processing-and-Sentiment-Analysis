{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:40:37.854221Z",
     "iopub.status.busy": "2024-08-22T10:40:37.853773Z",
     "iopub.status.idle": "2024-08-22T10:40:38.337105Z",
     "shell.execute_reply": "2024-08-22T10:40:38.335512Z",
     "shell.execute_reply.started": "2024-08-22T10:40:37.854178Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:40:45.065651Z",
     "iopub.status.busy": "2024-08-22T10:40:45.064691Z",
     "iopub.status.idle": "2024-08-22T10:40:46.600851Z",
     "shell.execute_reply": "2024-08-22T10:40:46.599514Z",
     "shell.execute_reply.started": "2024-08-22T10:40:45.065584Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/nr802/Downloads/GenAI Projects/IMDB Dataset.csv/IMDB Dataset.csv\",nrows= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:40:50.327742Z",
     "iopub.status.busy": "2024-08-22T10:40:50.327247Z",
     "iopub.status.idle": "2024-08-22T10:40:50.357577Z",
     "shell.execute_reply": "2024-08-22T10:40:50.356087Z",
     "shell.execute_reply.started": "2024-08-22T10:40:50.327696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"American Nightmare\" is officially tied, in my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>First off, I have to say that I loved the book...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>This movie was extremely boring. I only laughe...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>I was disgusted by this movie. No it wasn't be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    One of the other reviewers has mentioned that ...  positive\n",
       "1    A wonderful little production. <br /><br />The...  positive\n",
       "2    I thought this was a wonderful way to spend ti...  positive\n",
       "3    Basically there's a family where a little boy ...  negative\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "495  \"American Nightmare\" is officially tied, in my...  negative\n",
       "496  First off, I have to say that I loved the book...  negative\n",
       "497  This movie was extremely boring. I only laughe...  negative\n",
       "498  I was disgusted by this movie. No it wasn't be...  negative\n",
       "499  Such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:41:33.816352Z",
     "iopub.status.busy": "2024-08-22T10:41:33.815524Z",
     "iopub.status.idle": "2024-08-22T10:41:34.043572Z",
     "shell.execute_reply": "2024-08-22T10:41:34.042200Z",
     "shell.execute_reply.started": "2024-08-22T10:41:33.816303Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:41:47.452265Z",
     "iopub.status.busy": "2024-08-22T10:41:47.451813Z",
     "iopub.status.idle": "2024-08-22T10:41:47.468159Z",
     "shell.execute_reply": "2024-08-22T10:41:47.466839Z",
     "shell.execute_reply.started": "2024-08-22T10:41:47.452220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"american nightmare\" is officially tied, in my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>first off, i have to say that i loved the book...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>this movie was extremely boring. i only laughe...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>i was disgusted by this movie. no it wasn't be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one of the other reviewers has mentioned that ...  positive\n",
       "1    a wonderful little production. <br /><br />the...  positive\n",
       "2    i thought this was a wonderful way to spend ti...  positive\n",
       "3    basically there's a family where a little boy ...  negative\n",
       "4    petter mattei's \"love in the time of money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "495  \"american nightmare\" is officially tied, in my...  negative\n",
       "496  first off, i have to say that i loved the book...  negative\n",
       "497  this movie was extremely boring. i only laughe...  negative\n",
       "498  i was disgusted by this movie. no it wasn't be...  negative\n",
       "499  such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:42:14.232879Z",
     "iopub.status.busy": "2024-08-22T10:42:14.232388Z",
     "iopub.status.idle": "2024-08-22T10:42:14.241334Z",
     "shell.execute_reply": "2024-08-22T10:42:14.239528Z",
     "shell.execute_reply.started": "2024-08-22T10:42:14.232835Z"
    }
   },
   "outputs": [],
   "source": [
    "import re  #Regular Expressions\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:42:43.754796Z",
     "iopub.status.busy": "2024-08-22T10:42:43.754267Z",
     "iopub.status.idle": "2024-08-22T10:42:44.021398Z",
     "shell.execute_reply": "2024-08-22T10:42:44.020202Z",
     "shell.execute_reply.started": "2024-08-22T10:42:43.754751Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:42:52.767139Z",
     "iopub.status.busy": "2024-08-22T10:42:52.766697Z",
     "iopub.status.idle": "2024-08-22T10:42:52.782031Z",
     "shell.execute_reply": "2024-08-22T10:42:52.780742Z",
     "shell.execute_reply.started": "2024-08-22T10:42:52.767098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"american nightmare\" is officially tied, in my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>first off, i have to say that i loved the book...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>this movie was extremely boring. i only laughe...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>i was disgusted by this movie. no it wasn't be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one of the other reviewers has mentioned that ...  positive\n",
       "1    a wonderful little production. the filming tec...  positive\n",
       "2    i thought this was a wonderful way to spend ti...  positive\n",
       "3    basically there's a family where a little boy ...  negative\n",
       "4    petter mattei's \"love in the time of money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "495  \"american nightmare\" is officially tied, in my...  negative\n",
       "496  first off, i have to say that i loved the book...  negative\n",
       "497  this movie was extremely boring. i only laughe...  negative\n",
       "498  i was disgusted by this movie. no it wasn't be...  negative\n",
       "499  such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:43:13.398913Z",
     "iopub.status.busy": "2024-08-22T10:43:13.398392Z",
     "iopub.status.idle": "2024-08-22T10:43:13.404737Z",
     "shell.execute_reply": "2024-08-22T10:43:13.403476Z",
     "shell.execute_reply.started": "2024-08-22T10:43:13.398868Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www.\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:43:22.769802Z",
     "iopub.status.busy": "2024-08-22T10:43:22.769360Z",
     "iopub.status.idle": "2024-08-22T10:43:24.531711Z",
     "shell.execute_reply": "2024-08-22T10:43:24.530539Z",
     "shell.execute_reply.started": "2024-08-22T10:43:22.769760Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:43:34.909839Z",
     "iopub.status.busy": "2024-08-22T10:43:34.909319Z",
     "iopub.status.idle": "2024-08-22T10:43:34.922891Z",
     "shell.execute_reply": "2024-08-22T10:43:34.921673Z",
     "shell.execute_reply.started": "2024-08-22T10:43:34.909790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"american nightmare\" is officially tied, in my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>first off, i have to say that i loved the book...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>this movie was extremely boring. i only laughe...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>i was disgusted by this movie. no it wasn't be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one of the other reviewers has mentioned that ...  positive\n",
       "1    a wonderful little production. the filming tec...  positive\n",
       "2    i thought this was a wonderful way to spend ti...  positive\n",
       "3    basically there's a family where a little boy ...  negative\n",
       "4    petter mattei's \"love in the time of money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "495  \"american nightmare\" is officially tied, in my...  negative\n",
       "496  first off, i have to say that i loved the book...  negative\n",
       "497  this movie was extremely boring. i only laughe...  negative\n",
       "498  i was disgusted by this movie. no it wasn't be...  negative\n",
       "499  such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:43:59.575963Z",
     "iopub.status.busy": "2024-08-22T10:43:59.575543Z",
     "iopub.status.idle": "2024-08-22T10:43:59.583077Z",
     "shell.execute_reply": "2024-08-22T10:43:59.581776Z",
     "shell.execute_reply.started": "2024-08-22T10:43:59.575922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:44:09.716090Z",
     "iopub.status.busy": "2024-08-22T10:44:09.715675Z",
     "iopub.status.idle": "2024-08-22T10:44:09.721125Z",
     "shell.execute_reply": "2024-08-22T10:44:09.719676Z",
     "shell.execute_reply.started": "2024-08-22T10:44:09.716049Z"
    }
   },
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:44:24.572711Z",
     "iopub.status.busy": "2024-08-22T10:44:24.571870Z",
     "iopub.status.idle": "2024-08-22T10:44:24.577677Z",
     "shell.execute_reply": "2024-08-22T10:44:24.576506Z",
     "shell.execute_reply.started": "2024-08-22T10:44:24.572664Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:44:36.918759Z",
     "iopub.status.busy": "2024-08-22T10:44:36.917329Z",
     "iopub.status.idle": "2024-08-22T10:44:38.358548Z",
     "shell.execute_reply": "2024-08-22T10:44:38.357232Z",
     "shell.execute_reply.started": "2024-08-22T10:44:36.918635Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:44:47.655058Z",
     "iopub.status.busy": "2024-08-22T10:44:47.654614Z",
     "iopub.status.idle": "2024-08-22T10:44:47.669859Z",
     "shell.execute_reply": "2024-08-22T10:44:47.668549Z",
     "shell.execute_reply.started": "2024-08-22T10:44:47.655017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>american nightmare is officially tied in my op...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>first off i have to say that i loved the book ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>this movie was extremely boring i only laughed...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>i was disgusted by this movie no it wasnt beca...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one of the other reviewers has mentioned that ...  positive\n",
       "1    a wonderful little production the filming tech...  positive\n",
       "2    i thought this was a wonderful way to spend ti...  positive\n",
       "3    basically theres a family where a little boy j...  negative\n",
       "4    petter matteis love in the time of money is a ...  positive\n",
       "..                                                 ...       ...\n",
       "495  american nightmare is officially tied in my op...  negative\n",
       "496  first off i have to say that i loved the book ...  negative\n",
       "497  this movie was extremely boring i only laughed...  negative\n",
       "498  i was disgusted by this movie no it wasnt beca...  negative\n",
       "499  such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:45:43.190124Z",
     "iopub.status.busy": "2024-08-22T10:45:43.189662Z",
     "iopub.status.idle": "2024-08-22T10:45:44.671680Z",
     "shell.execute_reply": "2024-08-22T10:45:44.670516Z",
     "shell.execute_reply.started": "2024-08-22T10:45:43.190081Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:46:01.354880Z",
     "iopub.status.busy": "2024-08-22T10:46:01.354253Z",
     "iopub.status.idle": "2024-08-22T10:46:01.375607Z",
     "shell.execute_reply": "2024-08-22T10:46:01.374312Z",
     "shell.execute_reply.started": "2024-08-22T10:46:01.354836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:46:26.513943Z",
     "iopub.status.busy": "2024-08-22T10:46:26.513509Z",
     "iopub.status.idle": "2024-08-22T10:46:26.521481Z",
     "shell.execute_reply": "2024-08-22T10:46:26.520227Z",
     "shell.execute_reply.started": "2024-08-22T10:46:26.513902Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T10:46:39.263072Z",
     "iopub.status.busy": "2024-08-22T10:46:39.262590Z",
     "iopub.status.idle": "2024-08-22T11:13:14.669672Z",
     "shell.execute_reply": "2024-08-22T11:13:14.668255Z",
     "shell.execute_reply.started": "2024-08-22T10:46:39.263017Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.672946Z",
     "iopub.status.busy": "2024-08-22T11:13:14.672543Z",
     "iopub.status.idle": "2024-08-22T11:13:14.690834Z",
     "shell.execute_reply": "2024-08-22T11:13:14.689608Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.672904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one    reviewers  mentioned   watching  1 oz e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production  filming techniqu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought    wonderful way  spend time    hot s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres  family   little boy jake thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love   time  money   visually s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>american nightmare  officially tied   opinion ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>first     say   loved  book animal farm  read ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>movie  extremely boring   laughed   times  de...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>disgusted   movie   wasnt    graphic sex sce...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>joyous world   created  us  pixars  bugs lif...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    one    reviewers  mentioned   watching  1 oz e...  positive\n",
       "1     wonderful little production  filming techniqu...  positive\n",
       "2     thought    wonderful way  spend time    hot s...  positive\n",
       "3    basically theres  family   little boy jake thi...  negative\n",
       "4    petter matteis love   time  money   visually s...  positive\n",
       "..                                                 ...       ...\n",
       "495  american nightmare  officially tied   opinion ...  negative\n",
       "496  first     say   loved  book animal farm  read ...  negative\n",
       "497   movie  extremely boring   laughed   times  de...  negative\n",
       "498    disgusted   movie   wasnt    graphic sex sce...  negative\n",
       "499    joyous world   created  us  pixars  bugs lif...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Emojis\n",
    "\n",
    "- If we have emojis in the text, we have two options:\n",
    "  - Removing Emojis. (OR)\n",
    "  - Replacing emojis in the form of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.692982Z",
     "iopub.status.busy": "2024-08-22T11:13:14.692499Z",
     "iopub.status.idle": "2024-08-22T11:13:14.703012Z",
     "shell.execute_reply": "2024-08-22T11:13:14.701921Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.692928Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"u\\U0001F600-\\U0001F64F\"  # Emotions\n",
    "                               \"u\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               \"u\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               \"u\\U0001F1E0-\\U0001F1FF\"  # Flags(iOS)\n",
    "                               \"u\\U00002702-\\U000027B0\"\n",
    "                               \"u\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.707127Z",
     "iopub.status.busy": "2024-08-22T11:13:14.706552Z",
     "iopub.status.idle": "2024-08-22T11:13:14.724149Z",
     "shell.execute_reply": "2024-08-22T11:13:14.722811Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.707069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie, it was '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie, it was üëç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\nr802\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.726062Z",
     "iopub.status.busy": "2024-08-22T11:13:14.725649Z",
     "iopub.status.idle": "2024-08-22T11:13:14.814600Z",
     "shell.execute_reply": "2024-08-22T11:13:14.813272Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.726007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is üî•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using the split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.816686Z",
     "iopub.status.busy": "2024-08-22T11:13:14.816070Z",
     "iopub.status.idle": "2024-08-22T11:13:14.824811Z",
     "shell.execute_reply": "2024-08-22T11:13:14.823428Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.816637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "a = 'I am going to delhi'\n",
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.827530Z",
     "iopub.status.busy": "2024-08-22T11:13:14.826661Z",
     "iopub.status.idle": "2024-08-22T11:13:14.839747Z",
     "shell.execute_reply": "2024-08-22T11:13:14.838518Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.827468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iam going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "b = 'Iam going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "b.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.841627Z",
     "iopub.status.busy": "2024-08-22T11:13:14.841208Z",
     "iopub.status.idle": "2024-08-22T11:13:14.858182Z",
     "shell.execute_reply": "2024-08-22T11:13:14.856962Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.841575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem with split function\n",
    "c = 'I am going to delhi'\n",
    "c.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.860274Z",
     "iopub.status.busy": "2024-08-22T11:13:14.859835Z",
     "iopub.status.idle": "2024-08-22T11:13:14.871754Z",
     "shell.execute_reply": "2024-08-22T11:13:14.870422Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.860231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 'Where do think I should go? I have 3 day holiday'\n",
    "d.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.877840Z",
     "iopub.status.busy": "2024-08-22T11:13:14.877372Z",
     "iopub.status.idle": "2024-08-22T11:13:14.887085Z",
     "shell.execute_reply": "2024-08-22T11:13:14.885955Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.877796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "e = 'I am going to delhi'\n",
    "tokens = re.findall(\"[\\w']+\", e)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.888882Z",
     "iopub.status.busy": "2024-08-22T11:13:14.888397Z",
     "iopub.status.idle": "2024-08-22T11:13:14.901882Z",
     "shell.execute_reply": "2024-08-22T11:13:14.900593Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.888830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.903647Z",
     "iopub.status.busy": "2024-08-22T11:13:14.903216Z",
     "iopub.status.idle": "2024-08-22T11:13:14.914591Z",
     "shell.execute_reply": "2024-08-22T11:13:14.913186Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.903581Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.916650Z",
     "iopub.status.busy": "2024-08-22T11:13:14.916199Z",
     "iopub.status.idle": "2024-08-22T11:13:14.946757Z",
     "shell.execute_reply": "2024-08-22T11:13:14.945531Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.916605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'goind', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am goind to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.948573Z",
     "iopub.status.busy": "2024-08-22T11:13:14.948140Z",
     "iopub.status.idle": "2024-08-22T11:13:14.959157Z",
     "shell.execute_reply": "2024-08-22T11:13:14.957923Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.948531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.960985Z",
     "iopub.status.busy": "2024-08-22T11:13:14.960606Z",
     "iopub.status.idle": "2024-08-22T11:13:14.971453Z",
     "shell.execute_reply": "2024-08-22T11:13:14.970197Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.960945Z"
    }
   },
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're  here to help! mail us at gks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.973268Z",
     "iopub.status.busy": "2024-08-22T11:13:14.972902Z",
     "iopub.status.idle": "2024-08-22T11:13:14.987056Z",
     "shell.execute_reply": "2024-08-22T11:13:14.985141Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.973214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:14.989048Z",
     "iopub.status.busy": "2024-08-22T11:13:14.988516Z",
     "iopub.status.idle": "2024-08-22T11:13:15.003205Z",
     "shell.execute_reply": "2024-08-22T11:13:15.001724Z",
     "shell.execute_reply.started": "2024-08-22T11:13:14.988982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'gks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:15.005924Z",
     "iopub.status.busy": "2024-08-22T11:13:15.005409Z",
     "iopub.status.idle": "2024-08-22T11:13:15.015046Z",
     "shell.execute_reply": "2024-08-22T11:13:15.013768Z",
     "shell.execute_reply.started": "2024-08-22T11:13:15.005881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:15.017123Z",
     "iopub.status.busy": "2024-08-22T11:13:15.016730Z",
     "iopub.status.idle": "2024-08-22T11:13:19.910385Z",
     "shell.execute_reply": "2024-08-22T11:13:19.909111Z",
     "shell.execute_reply.started": "2024-08-22T11:13:15.017082Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #english small dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:19.913285Z",
     "iopub.status.busy": "2024-08-22T11:13:19.912336Z",
     "iopub.status.idle": "2024-08-22T11:13:19.969920Z",
     "shell.execute_reply": "2024-08-22T11:13:19.968749Z",
     "shell.execute_reply.started": "2024-08-22T11:13:19.913226Z"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:19.972476Z",
     "iopub.status.busy": "2024-08-22T11:13:19.971970Z",
     "iopub.status.idle": "2024-08-22T11:13:19.978770Z",
     "shell.execute_reply": "2024-08-22T11:13:19.977634Z",
     "shell.execute_reply.started": "2024-08-22T11:13:19.972394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc1: # here it is failing\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:20.025880Z",
     "iopub.status.busy": "2024-08-22T11:13:20.025404Z",
     "iopub.status.idle": "2024-08-22T11:13:20.033578Z",
     "shell.execute_reply": "2024-08-22T11:13:20.032188Z",
     "shell.execute_reply.started": "2024-08-22T11:13:20.025817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      " \n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "gks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:20.035426Z",
     "iopub.status.busy": "2024-08-22T11:13:20.034989Z",
     "iopub.status.idle": "2024-08-22T11:13:20.046581Z",
     "shell.execute_reply": "2024-08-22T11:13:20.045281Z",
     "shell.execute_reply.started": "2024-08-22T11:13:20.035342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:13:20.048450Z",
     "iopub.status.busy": "2024-08-22T11:13:20.048065Z",
     "iopub.status.idle": "2024-08-22T11:13:20.057823Z",
     "shell.execute_reply": "2024-08-22T11:13:20.056531Z",
     "shell.execute_reply.started": "2024-08-22T11:13:20.048394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "goind\n",
      "to\n",
      "visit\n",
      "delhi\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:57:03.520329Z",
     "iopub.status.busy": "2024-08-22T11:57:03.519101Z",
     "iopub.status.idle": "2024-08-22T11:57:05.927557Z",
     "shell.execute_reply": "2024-08-22T11:57:05.925987Z",
     "shell.execute_reply.started": "2024-08-22T11:57:03.520268Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:57:05.930795Z",
     "iopub.status.busy": "2024-08-22T11:57:05.930029Z",
     "iopub.status.idle": "2024-08-22T11:57:05.938115Z",
     "shell.execute_reply": "2024-08-22T11:57:05.936503Z",
     "shell.execute_reply.started": "2024-08-22T11:57:05.930722Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run jump happili studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Specify the language (English in this case)\n",
    "ss = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ss.stem(word) for word in text.split()])\n",
    "# # Example usage\n",
    "text = \"running jumped happily studies\"\n",
    "print(stem_words(text))  # Output: run jump happi studi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run jump happy study\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ls.stem(word) for word in text.split()])\n",
    "\n",
    "# Example usage\n",
    "text = \"running jumped happily studies\"\n",
    "print(stem_words(text))  # Output: run jump happy study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def regex_stem(word):\n",
    "    return re.sub(r'(ing|ed|ly|es|s)$', '', word)\n",
    "\n",
    "print(regex_stem(\"running\"))  # Output: runn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn jump happi studi\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def regex_stem(text):\n",
    "    return \" \".join([re.sub(r'(ing|ed|ly|es|s)$', '', word) for word in text.split()])\n",
    "\n",
    "# Example usage\n",
    "text = \"running jumped happily studies\"\n",
    "print(regex_stem(text))  # Output: run jump happi studi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Original': 'running', 'Porter': 'run', 'Lancaster': 'run', 'Snowball': 'run'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "ss = SnowballStemmer(\"english\")\n",
    "\n",
    "# Function to apply all stemmers\n",
    "def apply_stemmers(word):\n",
    "    return {\n",
    "        \"Original\": word,\n",
    "        \"Porter\": ps.stem(word),\n",
    "        \"Lancaster\": ls.stem(word),\n",
    "        \"Snowball\": ss.stem(word)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "word = \"running\"\n",
    "print(apply_stemmers(word))\n",
    "# Output: {'Original': 'running', 'Porter': 'run', 'Lancaster': 'run', 'Snowball': 'run'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:57:11.529978Z",
     "iopub.status.busy": "2024-08-22T11:57:11.528790Z",
     "iopub.status.idle": "2024-08-22T11:57:11.539029Z",
     "shell.execute_reply": "2024-08-22T11:57:11.537501Z",
     "shell.execute_reply.started": "2024-08-22T11:57:11.529920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:57:29.573789Z",
     "iopub.status.busy": "2024-08-22T11:57:29.573245Z",
     "iopub.status.idle": "2024-08-22T11:57:29.581944Z",
     "shell.execute_reply": "2024-08-22T11:57:29.580247Z",
     "shell.execute_reply.started": "2024-08-22T11:57:29.573695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my all time favourite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen some 15 or more times in the last 25 years.\n"
     ]
    }
   ],
   "source": [
    "text = \"probably my all time favourite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen some 15 or more times in the last 25 years.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:57:45.785343Z",
     "iopub.status.busy": "2024-08-22T11:57:45.784784Z",
     "iopub.status.idle": "2024-08-22T11:57:45.795964Z",
     "shell.execute_reply": "2024-08-22T11:57:45.794440Z",
     "shell.execute_reply.started": "2024-08-22T11:57:45.785288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prob my al tim favourit movy a story of selfless sacr and ded to a nobl caus but it not preachy or bor it just nev get old despit my hav seen som 15 or mor tim in the last 25 years.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here after doing stemming the words like probably, movie, sacrifice etc are converted to probabl, movi, sacrific\n",
    "- To solve this issue we will do Lemmetization.\n",
    "- Work of both Stemming and Lemmetization are same.\n",
    "- The difference is, Lemmetization is slow compared to Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:58:41.022243Z",
     "iopub.status.busy": "2024-08-22T11:58:41.021774Z",
     "iopub.status.idle": "2024-08-22T11:58:41.028408Z",
     "shell.execute_reply": "2024-08-22T11:58:41.027008Z",
     "shell.execute_reply.started": "2024-08-22T11:58:41.022200Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T12:02:42.947333Z",
     "iopub.status.busy": "2024-08-22T12:02:42.946869Z",
     "iopub.status.idle": "2024-08-22T12:02:42.958446Z",
     "shell.execute_reply": "2024-08-22T12:02:42.956832Z",
     "shell.execute_reply.started": "2024-08-22T12:02:42.947290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# You might also need to download the 'omw-1.4' package for wordnet\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "aws                 aws                 \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He aws running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations = \"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "    \n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv(\"C:/Users/nr802/Downloads/GenAI Projects/IMDB Dataset.csv/IMDB Dataset.csv\", sep=',', chunksize=200)\n",
    "\n",
    "# Get the first chunk and access its columns\n",
    "df = next(chunks)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"words\"]=\"default value\"\n",
    "df[\"sentences\"]=\"default value\"\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,\"words\"]= list(\"\")\n",
    "    df.at[i,\"sentences\"] = list(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    l1= sent_tokenize(str(df.loc[i,\"review\"]))\n",
    "    df.at[i,\"sentences\"]=l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\nr802/nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\nr802\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nr802\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger', download_dir='C:\\\\Users\\\\nr802\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nr802\\nltk_data\\taggers\\averaged_perceptron_tagger\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.find('taggers/averaged_perceptron_tagger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\nr802/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize sentences\n",
    "def lemmatize_with_nltk(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply the custom lemmatizer\n",
    "for k in range(df.shape[0]):\n",
    "    df.at[k, \"words\"] = []\n",
    "    for sentence in df.loc[k, \"sentences\"]:\n",
    "        lemmatized_words = lemmatize_with_nltk(sentence)\n",
    "        df.at[k, \"words\"].extend(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"words_sentences\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "for k in range(df.shape[0]):\n",
    "    words_list = df.loc[k, \"words\"]\n",
    "    if words_list:  # Check if the list is not empty\n",
    "        df.loc[k, \"words_sentences\"] = \" \".join(words_list)  # Use join() for better readability\n",
    "    else:\n",
    "        df.loc[k, \"words_sentences\"] = \"\"  # Assign empty string if the list is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import  CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df\n",
    "\n",
    "# no_features = 1000\n",
    "# tf_vectorizer = CountVectorizer( max_features=no_features, stop_words='english')\n",
    "# tf = tf_vectorizer.fit_transform(df1.words_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x=pd.DataFrame(tf.A, columns=tf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df.words_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_1=pd.DataFrame(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_enc=df_y_1.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentiment'], dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_enc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          0\n",
       "4          1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nr802\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(dense,df_y_enc)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = rf.score(dense,df_y_enc)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 94.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nr802\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dense,df_y_enc)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_nb = nb.score(dense,df_y_enc)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC=GradientBoostingClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nr802\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "gb_c=GBC.fit(dense,df_y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc_score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "gbc_score=GBC.score(dense,df_y_enc)\n",
    "print(f\"gbc_score: {gbc_score* 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/nr802/Downloads/GenAI Projects/IMDB Dataset.csv/IMDB Dataset.csv\",nrows=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'text' and 'label' columns exist (modify based on actual column names)\n",
    "# Assuming 'text' column contains reviews and 'label' contains sentiment\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "df = df.dropna(subset=['review', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7666666666666667\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.90      0.81        67\n",
      "    positive       0.82      0.60      0.70        53\n",
      "\n",
      "    accuracy                           0.77       120\n",
      "   macro avg       0.78      0.75      0.75       120\n",
      "weighted avg       0.78      0.77      0.76       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing and vectorization\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.79      0.82        67\n",
      "    positive       0.75      0.81      0.78        53\n",
      "\n",
      "    accuracy                           0.80       120\n",
      "   macro avg       0.80      0.80      0.80       120\n",
      "weighted avg       0.80      0.80      0.80       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Assuming 'reviewText' is the column with reviews and 'verified' is the sentiment label (or your target column)\n",
    "# Preprocessing: Text vectorization (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(df['review'])\n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)  # Correct target column\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=df['sentiment'].unique(), y=df['sentiment'])  # Corrected target column\n",
    "class_weight_dict = dict(zip(df['sentiment'].unique(), class_weights))  # Creating the class weight dictionary\n",
    "\n",
    "# Train a Logistic Regression model with class weights\n",
    "model = LogisticRegression(class_weight=class_weight_dict)  # Corrected syntax\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
